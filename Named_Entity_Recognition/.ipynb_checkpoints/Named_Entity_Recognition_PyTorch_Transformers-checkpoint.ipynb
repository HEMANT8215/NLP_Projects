{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "doGOemX5kkmZ",
    "outputId": "b6ce30d3-eb0a-4be2-f85b-ae3f86755670"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Emotions.txt\t   ner_dataset.csv\t\t   theta.txt\n",
      "glove.6B.200d.txt  NHC_data_Biju.xlsx\t\t   tokenizer_config.json\n",
      "glove.6B.50d.txt   Quora_question_pairs_test.csv   vocab.txt\n",
      "model.png\t   Quora_question_pairs_train.csv\n",
      "ner.csv\t\t   special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#import os\n",
    "#os.chdir(\"/content/drive/MyDrive/DATA\")\n",
    "#!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WmvOalFbs2WL"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4c2f527c12ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNAMfp3p1UET"
   },
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOpAFgtkksjV"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../ner_dataset.csv', encoding= 'unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "FUWTPIz8ksy1",
    "outputId": "4b7f8728-7699-4e7d-b743-2beed8f8b860"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1          NaN             of   IN   O\n",
       "2          NaN  demonstrators  NNS   O\n",
       "3          NaN           have  VBP   O\n",
       "4          NaN        marched  VBN   O"
      ]
     },
     "execution_count": 84,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "nx4U6B2eqkDt"
   },
   "outputs": [],
   "source": [
    "count_tags = df['Tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W71ASbVzaS7p",
    "outputId": "644ff8bb-f00f-4579-8c4d-5285e30aa18b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O        887908\n",
       "B-geo     37644\n",
       "B-tim     20333\n",
       "B-org     20143\n",
       "I-per     17251\n",
       "B-per     16990\n",
       "I-org     16784\n",
       "B-gpe     15870\n",
       "I-geo      7414\n",
       "I-tim      6528\n",
       "B-art       402\n",
       "B-eve       308\n",
       "I-art       297\n",
       "I-eve       253\n",
       "B-nat       201\n",
       "I-gpe       198\n",
       "I-nat        51\n",
       "Name: Tag, dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z2AoUOPjZU4j",
    "outputId": "9ae25e8b-53f6-44a6-80aa-4a4db71e1dac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "903778"
      ]
     },
     "execution_count": 87,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tags['O'] + count_tags[\"B-gpe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "-6DMGD8D_w1H"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "#df['Word'] = df['Word'].str.translate(str.maketrans('','',string.punctuation))\n",
    "df['Word'] = df['Word'].str.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hp_LNGT3CilR"
   },
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "lzkXgliCCdra"
   },
   "outputs": [],
   "source": [
    "## upload word emebedding file on google drive\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words, words_to_index, index_to_words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "0LmDGbRXCduM"
   },
   "outputs": [],
   "source": [
    "words, word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s63a8gX9Cdww",
    "outputId": "064be5ec-b4be-4d43-d0e5-ff4fc926f08a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400002, 50)\n"
     ]
    }
   ],
   "source": [
    "vocab_len = len(word_to_index) + 2            \n",
    "emb_dim = word_to_vec_map[\"cucumber\"].shape[0]  \n",
    "emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "for word, index in word_to_index.items():\n",
    "    emb_matrix[index, :] = word_to_vec_map[word]\n",
    "print(emb_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1UmVu7Sv1fG5"
   },
   "source": [
    "# Extract mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "C0VxQ7Z6lP9s"
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "def get_dict_map(data, token_or_tag):\n",
    "    tok2idx = {}\n",
    "    idx2tok = {}\n",
    "    \n",
    "    if token_or_tag == 'token':\n",
    "        n=2\n",
    "        vocab = list(set(data['Word'].to_list()))\n",
    "    else:\n",
    "        n=0\n",
    "        vocab = list(set(data['Tag'].to_list()))\n",
    "\n",
    "\n",
    "    idx2tok = {idx:tok for  idx, tok in enumerate(vocab, n)}\n",
    "    tok2idx = {tok:idx for  idx, tok in enumerate(vocab, n)}\n",
    "    return tok2idx, idx2tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "1YGvBLrHlYOa"
   },
   "outputs": [],
   "source": [
    "token2idx, idx2token = get_dict_map(df, 'token')\n",
    "tag2idx, idx2tag = get_dict_map(df, 'tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "mKvaPo8r3wJm"
   },
   "outputs": [],
   "source": [
    "token2idx['<PAD>'] = 1\n",
    "idx2token[1] = '<PAD>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "c1mjoDlZ4zmS"
   },
   "outputs": [],
   "source": [
    "token2idx['UNK'] = 0\n",
    "idx2token[0] = '<UNK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "YSi3hamylYXe",
    "outputId": "851a2b69-36f4-478e-c727-4d501cfa7661"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Word_idx</th>\n",
       "      <th>Tag_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>358481.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>268046.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>120649.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "      <td>174642.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "      <td>234138.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag  Word_idx  Tag_idx\n",
       "0  Sentence: 1      thousands  NNS   O  358481.0       11\n",
       "1          NaN             of   IN   O  268046.0       11\n",
       "2          NaN  demonstrators  NNS   O  120649.0       11\n",
       "3          NaN           have  VBP   O  174642.0       11\n",
       "4          NaN        marched  VBN   O  234138.0       11"
      ]
     },
     "execution_count": 96,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Word_idx'] = df['Word'].map(word_to_index)\n",
    "df['Tag_idx'] = df['Tag'].map(tag2idx)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "J3fyFgIfCFQN"
   },
   "outputs": [],
   "source": [
    "unique_tokens = len(word_to_index) # len(list(set(data['Word'].to_list())))\n",
    "unique_tags = len(list(set(df['Tag'].to_list())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "onbSUp4mpG-7"
   },
   "outputs": [],
   "source": [
    "df_fill = df.fillna(method='ffill', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "y-9Eh6dipWKU",
    "outputId": "029a5b72-0d73-4c64-f0b5-e94038d2cb3a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Word_idx</th>\n",
       "      <th>Tag_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>358481.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>268046.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>120649.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "      <td>174642.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "      <td>234138.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag  Word_idx  Tag_idx\n",
       "0  Sentence: 1      thousands  NNS   O  358481.0       11\n",
       "1  Sentence: 1             of   IN   O  268046.0       11\n",
       "2  Sentence: 1  demonstrators  NNS   O  120649.0       11\n",
       "3  Sentence: 1           have  VBP   O  174642.0       11\n",
       "4  Sentence: 1        marched  VBN   O  234138.0       11"
      ]
     },
     "execution_count": 99,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fill.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UBpKPh4fpipt",
    "outputId": "4f80afd7-63d6-40ff-b7bd-687297a33295"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_group = df_fill.groupby(['Sentence #'],as_index=False)['Word', 'POS', 'Tag', 'Word_idx', 'Tag_idx'].agg(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "8s9eHeSJp8bg",
    "outputId": "868ed8ae-e938-40e5-f576-3154e11e42c1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Word_idx</th>\n",
       "      <th>Tag_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>[thousands, of, demonstrators, have, marched, ...</td>\n",
       "      <td>[NNS, IN, NNS, VBP, VBN, IN, NNP, TO, VB, DT, ...</td>\n",
       "      <td>[O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...</td>\n",
       "      <td>[358481.0, 268046.0, 120649.0, 174642.0, 23413...</td>\n",
       "      <td>[11, 11, 11, 11, 11, 11, 14, 11, 11, 11, 11, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 10</td>\n",
       "      <td>[iranian, officials, say, they, expect, to, ge...</td>\n",
       "      <td>[JJ, NNS, VBP, PRP, VBP, TO, VB, NN, TO, JJ, J...</td>\n",
       "      <td>[B-gpe, O, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>[192569.0, 268225.0, 319691.0, 357810.0, 14275...</td>\n",
       "      <td>[4, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 100</td>\n",
       "      <td>[helicopter, gunships, saturday, pounded, mili...</td>\n",
       "      <td>[NN, NNS, NNP, VBD, JJ, NNS, IN, DT, NNP, JJ, ...</td>\n",
       "      <td>[O, O, B-tim, O, O, O, O, O, B-geo, O, O, O, O...</td>\n",
       "      <td>[176329.0, 169259.0, 319134.0, 290058.0, 24443...</td>\n",
       "      <td>[11, 11, 12, 11, 11, 11, 11, 11, 14, 11, 11, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1000</td>\n",
       "      <td>[they, left, after, a, tense, hour-long, stand...</td>\n",
       "      <td>[PRP, VBD, IN, DT, NN, JJ, NN, IN, NN, NNS, .]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[357810.0, 219577.0, 47798.0, 43010.0, 355882....</td>\n",
       "      <td>[11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 10000</td>\n",
       "      <td>[u.n., relief, coordinator, jan, egeland, said...</td>\n",
       "      <td>[NNP, NN, NN, NNP, NNP, VBD, NNP, ,, NNP, ,, J...</td>\n",
       "      <td>[B-geo, O, O, B-per, I-per, O, B-tim, O, B-geo...</td>\n",
       "      <td>[369144.0, 305053.0, 108674.0, 195458.0, 13404...</td>\n",
       "      <td>[14, 11, 11, 1, 6, 11, 12, 11, 14, 11, 4, 11, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sentence #  ...                                            Tag_idx\n",
       "0      Sentence: 1  ...  [11, 11, 11, 11, 11, 11, 14, 11, 11, 11, 11, 1...\n",
       "1     Sentence: 10  ...  [4, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11...\n",
       "2    Sentence: 100  ...  [11, 11, 12, 11, 11, 11, 11, 11, 14, 11, 11, 1...\n",
       "3   Sentence: 1000  ...       [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]\n",
       "4  Sentence: 10000  ...  [14, 11, 11, 1, 6, 11, 12, 11, 14, 11, 4, 11, ...\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_group.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmKRqjvIXiMs"
   },
   "source": [
    "# Data Loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F2iYmzAbSYGO",
    "outputId": "58ef3f9c-815a-45c5-ae61-92a048841fb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_tokens length: 33571 \n",
      "test_tokens length: 14388 \n",
      "train_tags: 33571 \n",
      "test_tags: 14388\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_pad_train_test_val(data_group, data):\n",
    "\n",
    "    #get max token and tag length\n",
    "    n_token = len(list(set(data['Word'].to_list())))\n",
    "    n_tag = len(list(set(data['Tag'].to_list())))\n",
    "\n",
    "    #Pad tokens (X var)    \n",
    "    tokens = data_group['Word_idx'].tolist()\n",
    "    maxlen = max([len(s) for s in tokens])\n",
    "    pad_tokens = pad_sequences(tokens, maxlen=maxlen, dtype='float32', padding='post', value= token2idx['<PAD>'])\n",
    "    torch_tokens = torch.tensor(pad_tokens, dtype=torch.long)\n",
    "\n",
    "\n",
    "    #Pad Tags (y var) and convert it into one hot encoding\n",
    "    tags = data_group['Tag_idx'].tolist()\n",
    "    pad_tags = pad_sequences(tags, maxlen=maxlen, dtype='float32', padding='post', value= tag2idx[\"O\"])\n",
    "    torch_tags = torch.tensor(pad_tags, dtype=torch.long)\n",
    "    #n_tags = len(tag2idx)\n",
    "    #pad_tags = [to_categorical(i, num_classes=n_tags) for i in pad_tags]\n",
    "    \n",
    "    #Split train, test and validation set\n",
    "    train_tokens, test_tokens, train_tags, test_tags = train_test_split(torch_tokens, torch_tags, test_size=0.3, train_size=0.7, random_state=2020)\n",
    "    \n",
    "\n",
    "    print(\n",
    "        'train_tokens length:', len(train_tokens),\n",
    "        '\\ntest_tokens length:', len(test_tokens),\n",
    "        '\\ntrain_tags:', len(train_tags),\n",
    "        '\\ntest_tags:', len(test_tags),\n",
    "    )\n",
    "    \n",
    "    return train_tokens, test_tokens, train_tags, test_tags\n",
    "\n",
    "train_tokens, test_tokens, train_tags, test_tags = get_pad_train_test_val(df_group, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "wAVMB_TzWyuJ"
   },
   "outputs": [],
   "source": [
    "class TextDataset(data.Dataset):\n",
    "    '''\n",
    "    Simple Dataset\n",
    "    '''\n",
    "    def __init__(self,X,y=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is not None:\n",
    "            return [self.X[idx].to(device),self.y[idx].to(device)]\n",
    "        return self.X[idx].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "sxch5JEqWyz-"
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "train_dataset = TextDataset(train_tokens,train_tags)\n",
    "test_dataset = TextDataset(test_tokens,test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "jANI5L2XWM7E"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "#for X,Y in train_loader:\n",
    "    #print(X.shape,Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QN-fZ2OmXyKy"
   },
   "source": [
    "# Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "4IN7F-jKCdzE"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "KKLqgWrYX7-x"
   },
   "outputs": [],
   "source": [
    "emb_matrix = torch.FloatTensor(emb_matrix)\n",
    "embedding = torch.nn.Embedding.from_pretrained(emb_matrix,freeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "e8JJzdtZX8Dp"
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, num_heads, hidden_dim, num_layers, num_labels, pad_idx, dropout=0.5, device=\"cuda\"):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.encoder = embedding # nn.Embedding(vocab_size, embedding_size)\n",
    "        self.pos_encoder = PositionalEncoding(embedding_size, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(embedding_size, num_heads, hidden_dim, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.fc_out = nn.Linear(embedding_size, num_labels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.pad_idx = pad_idx\n",
    "        self.embedding_size = embedding_size\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        # use src_mask to ignore pad indices embeddings while computing self-attention output\n",
    "        src_mask = src.transpose(0, 1) == self.pad_idx\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        src = self.encoder(src) * math.sqrt(self.embedding_size)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_key_padding_mask=src_mask)\n",
    "        output = self.fc_out(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "Pu5u9Lm1X8GN"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "N-tnb37sX8JA"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vocab_size = len(word_to_index) + 2\n",
    "num_classes = unique_tags\n",
    "embedding_size = 50\n",
    "num_heads = 5\n",
    "pad_idx = 1\n",
    "num_layers = 6\n",
    "dropout = 0.0\n",
    "hidden_dim = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "wD8svBz8X8Ls"
   },
   "outputs": [],
   "source": [
    "model = TransformerModel(vocab_size, embedding_size, num_heads,hidden_dim, num_layers, num_classes, pad_idx, dropout, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x9uv_qq-X8OY",
    "outputId": "6ebc380d-c9ac-4613-9b0b-8e8b7811863e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummaryX in /usr/local/lib/python3.7/dist-packages (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchsummaryX) (1.19.5)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchsummaryX) (1.8.1+cu101)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torchsummaryX) (1.1.5)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchsummaryX) (3.7.4.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->torchsummaryX) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torchsummaryX) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torchsummaryX) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummaryX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ffJCj_YaX8Rc",
    "outputId": "ff0bbcca-2ea1-4eee-e02e-5d6cd8646095"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================\n",
      "                                                  Kernel Shape   Output Shape  \\\n",
      "Layer                                                                           \n",
      "0_encoder                                         [50, 400002]    [20, 4, 50]   \n",
      "1_pos_encoder.Dropout_dropout                                -    [20, 4, 50]   \n",
      "2_transformer_encoder.layers.0.Dropout_dropout1              -    [20, 4, 50]   \n",
      "3_transformer_encoder.layers.0.LayerNorm_norm1            [50]    [20, 4, 50]   \n",
      "4_transformer_encoder.layers.0.Linear_linear1       [50, 2048]  [20, 4, 2048]   \n",
      "5_transformer_encoder.layers.0.Dropout_dropout               -  [20, 4, 2048]   \n",
      "6_transformer_encoder.layers.0.Linear_linear2       [2048, 50]    [20, 4, 50]   \n",
      "7_transformer_encoder.layers.0.Dropout_dropout2              -    [20, 4, 50]   \n",
      "8_transformer_encoder.layers.0.LayerNorm_norm2            [50]    [20, 4, 50]   \n",
      "9_transformer_encoder.layers.1.Dropout_dropout1              -    [20, 4, 50]   \n",
      "10_transformer_encoder.layers.1.LayerNorm_norm1           [50]    [20, 4, 50]   \n",
      "11_transformer_encoder.layers.1.Linear_linear1      [50, 2048]  [20, 4, 2048]   \n",
      "12_transformer_encoder.layers.1.Dropout_dropout              -  [20, 4, 2048]   \n",
      "13_transformer_encoder.layers.1.Linear_linear2      [2048, 50]    [20, 4, 50]   \n",
      "14_transformer_encoder.layers.1.Dropout_dropout2             -    [20, 4, 50]   \n",
      "15_transformer_encoder.layers.1.LayerNorm_norm2           [50]    [20, 4, 50]   \n",
      "16_transformer_encoder.layers.2.Dropout_dropout1             -    [20, 4, 50]   \n",
      "17_transformer_encoder.layers.2.LayerNorm_norm1           [50]    [20, 4, 50]   \n",
      "18_transformer_encoder.layers.2.Linear_linear1      [50, 2048]  [20, 4, 2048]   \n",
      "19_transformer_encoder.layers.2.Dropout_dropout              -  [20, 4, 2048]   \n",
      "20_transformer_encoder.layers.2.Linear_linear2      [2048, 50]    [20, 4, 50]   \n",
      "21_transformer_encoder.layers.2.Dropout_dropout2             -    [20, 4, 50]   \n",
      "22_transformer_encoder.layers.2.LayerNorm_norm2           [50]    [20, 4, 50]   \n",
      "23_transformer_encoder.layers.3.Dropout_dropout1             -    [20, 4, 50]   \n",
      "24_transformer_encoder.layers.3.LayerNorm_norm1           [50]    [20, 4, 50]   \n",
      "25_transformer_encoder.layers.3.Linear_linear1      [50, 2048]  [20, 4, 2048]   \n",
      "26_transformer_encoder.layers.3.Dropout_dropout              -  [20, 4, 2048]   \n",
      "27_transformer_encoder.layers.3.Linear_linear2      [2048, 50]    [20, 4, 50]   \n",
      "28_transformer_encoder.layers.3.Dropout_dropout2             -    [20, 4, 50]   \n",
      "29_transformer_encoder.layers.3.LayerNorm_norm2           [50]    [20, 4, 50]   \n",
      "30_transformer_encoder.layers.4.Dropout_dropout1             -    [20, 4, 50]   \n",
      "31_transformer_encoder.layers.4.LayerNorm_norm1           [50]    [20, 4, 50]   \n",
      "32_transformer_encoder.layers.4.Linear_linear1      [50, 2048]  [20, 4, 2048]   \n",
      "33_transformer_encoder.layers.4.Dropout_dropout              -  [20, 4, 2048]   \n",
      "34_transformer_encoder.layers.4.Linear_linear2      [2048, 50]    [20, 4, 50]   \n",
      "35_transformer_encoder.layers.4.Dropout_dropout2             -    [20, 4, 50]   \n",
      "36_transformer_encoder.layers.4.LayerNorm_norm2           [50]    [20, 4, 50]   \n",
      "37_transformer_encoder.layers.5.Dropout_dropout1             -    [20, 4, 50]   \n",
      "38_transformer_encoder.layers.5.LayerNorm_norm1           [50]    [20, 4, 50]   \n",
      "39_transformer_encoder.layers.5.Linear_linear1      [50, 2048]  [20, 4, 2048]   \n",
      "40_transformer_encoder.layers.5.Dropout_dropout              -  [20, 4, 2048]   \n",
      "41_transformer_encoder.layers.5.Linear_linear2      [2048, 50]    [20, 4, 50]   \n",
      "42_transformer_encoder.layers.5.Dropout_dropout2             -    [20, 4, 50]   \n",
      "43_transformer_encoder.layers.5.LayerNorm_norm2           [50]    [20, 4, 50]   \n",
      "44_fc_out                                             [50, 17]    [20, 4, 17]   \n",
      "\n",
      "                                                    Params Mult-Adds  \n",
      "Layer                                                                 \n",
      "0_encoder                                                -         -  \n",
      "1_pos_encoder.Dropout_dropout                            -         -  \n",
      "2_transformer_encoder.layers.0.Dropout_dropout1          -         -  \n",
      "3_transformer_encoder.layers.0.LayerNorm_norm1       100.0      50.0  \n",
      "4_transformer_encoder.layers.0.Linear_linear1     104.448k    102.4k  \n",
      "5_transformer_encoder.layers.0.Dropout_dropout           -         -  \n",
      "6_transformer_encoder.layers.0.Linear_linear2      102.45k    102.4k  \n",
      "7_transformer_encoder.layers.0.Dropout_dropout2          -         -  \n",
      "8_transformer_encoder.layers.0.LayerNorm_norm2       100.0      50.0  \n",
      "9_transformer_encoder.layers.1.Dropout_dropout1          -         -  \n",
      "10_transformer_encoder.layers.1.LayerNorm_norm1      100.0      50.0  \n",
      "11_transformer_encoder.layers.1.Linear_linear1    104.448k    102.4k  \n",
      "12_transformer_encoder.layers.1.Dropout_dropout          -         -  \n",
      "13_transformer_encoder.layers.1.Linear_linear2     102.45k    102.4k  \n",
      "14_transformer_encoder.layers.1.Dropout_dropout2         -         -  \n",
      "15_transformer_encoder.layers.1.LayerNorm_norm2      100.0      50.0  \n",
      "16_transformer_encoder.layers.2.Dropout_dropout1         -         -  \n",
      "17_transformer_encoder.layers.2.LayerNorm_norm1      100.0      50.0  \n",
      "18_transformer_encoder.layers.2.Linear_linear1    104.448k    102.4k  \n",
      "19_transformer_encoder.layers.2.Dropout_dropout          -         -  \n",
      "20_transformer_encoder.layers.2.Linear_linear2     102.45k    102.4k  \n",
      "21_transformer_encoder.layers.2.Dropout_dropout2         -         -  \n",
      "22_transformer_encoder.layers.2.LayerNorm_norm2      100.0      50.0  \n",
      "23_transformer_encoder.layers.3.Dropout_dropout1         -         -  \n",
      "24_transformer_encoder.layers.3.LayerNorm_norm1      100.0      50.0  \n",
      "25_transformer_encoder.layers.3.Linear_linear1    104.448k    102.4k  \n",
      "26_transformer_encoder.layers.3.Dropout_dropout          -         -  \n",
      "27_transformer_encoder.layers.3.Linear_linear2     102.45k    102.4k  \n",
      "28_transformer_encoder.layers.3.Dropout_dropout2         -         -  \n",
      "29_transformer_encoder.layers.3.LayerNorm_norm2      100.0      50.0  \n",
      "30_transformer_encoder.layers.4.Dropout_dropout1         -         -  \n",
      "31_transformer_encoder.layers.4.LayerNorm_norm1      100.0      50.0  \n",
      "32_transformer_encoder.layers.4.Linear_linear1    104.448k    102.4k  \n",
      "33_transformer_encoder.layers.4.Dropout_dropout          -         -  \n",
      "34_transformer_encoder.layers.4.Linear_linear2     102.45k    102.4k  \n",
      "35_transformer_encoder.layers.4.Dropout_dropout2         -         -  \n",
      "36_transformer_encoder.layers.4.LayerNorm_norm2      100.0      50.0  \n",
      "37_transformer_encoder.layers.5.Dropout_dropout1         -         -  \n",
      "38_transformer_encoder.layers.5.LayerNorm_norm1      100.0      50.0  \n",
      "39_transformer_encoder.layers.5.Linear_linear1    104.448k    102.4k  \n",
      "40_transformer_encoder.layers.5.Dropout_dropout          -         -  \n",
      "41_transformer_encoder.layers.5.Linear_linear2     102.45k    102.4k  \n",
      "42_transformer_encoder.layers.5.Dropout_dropout2         -         -  \n",
      "43_transformer_encoder.layers.5.LayerNorm_norm2      100.0      50.0  \n",
      "44_fc_out                                            867.0     850.0  \n",
      "--------------------------------------------------------------------------------------------------\n",
      "                          Totals\n",
      "Total params          21.243555M\n",
      "Trainable params       1.243455M\n",
      "Non-trainable params    20.0001M\n",
      "Mult-Adds               1.23025M\n",
      "==================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kernel Shape</th>\n",
       "      <th>Output Shape</th>\n",
       "      <th>Params</th>\n",
       "      <th>Mult-Adds</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0_encoder</th>\n",
       "      <td>[50, 400002]</td>\n",
       "      <td>[20, 4, 50]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_pos_encoder.Dropout_dropout</th>\n",
       "      <td>-</td>\n",
       "      <td>[20, 4, 50]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_transformer_encoder.layers.0.Dropout_dropout1</th>\n",
       "      <td>-</td>\n",
       "      <td>[20, 4, 50]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_transformer_encoder.layers.0.LayerNorm_norm1</th>\n",
       "      <td>[50]</td>\n",
       "      <td>[20, 4, 50]</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_transformer_encoder.layers.0.Linear_linear1</th>\n",
       "      <td>[50, 2048]</td>\n",
       "      <td>[20, 4, 2048]</td>\n",
       "      <td>104448.0</td>\n",
       "      <td>102400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5_transformer_encoder.layers.0.Dropout_dropout</th>\n",
       "      <td>-</td>\n",
       "      <td>[20, 4, 2048]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6_transformer_encoder.layers.0.Linear_linear2</th>\n",
       "      <td>[2048, 50]</td>\n",
       "      <td>[20, 4, 50]</td>\n",
       "      <td>102450.0</td>\n",
       "      <td>102400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7_transformer_encoder.layers.0.Dropout_dropout2</th>\n",
       "      <td>-</td>\n",
       "      <td>[20, 4, 50]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8_transformer_encoder.layers.0.LayerNorm_norm2</th>\n",
       "      <td>[50]</td>\n",
       "      <td>[20, 4, 50]</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9_transformer_encoder.layers.1.Dropout_dropout1</th>\n",
       "      <td>-</td>\n",
       "      <td>[20, 4, 50]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_transformer_encoder.layers.1.LayerNorm_norm1</th>\n",
       "      <td>[50]</td>\n",
       "      <td>[20, 4, 50]</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11_transformer_encoder.layers.1.Linear_linear1</th>\n",
       "      <td>[50, 2048]</td>\n",
       "      <td>[20, 4, 2048]</td>\n",
       "      <td>104448.0</td>\n",
       "      <td>102400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12_transformer_encoder.layers.1.Dropout_dropout</th>\n",
       "      <td>-</td>\n",
       "      <td>[20, 4, 2048]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13_transformer_encoder.layers.1.Linear_linear2</th>\n",
       "      <td>[2048, 50]</td>\n",
       "      <td>[20, 4, 50]</td>\n",
       "      <td>102450.0</td>\n",
       "      <td>102400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14_transformer_encoder.layers.1.Dropout_dropout2</th>\n",
       "      <td>-</td>\n",
       "      <td>[20, 4, 50]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15_transformer_encoder.layers.1.LayerNorm_norm2</th>\n",
       "      <td>[50]</td>\n",
       "      <td>[20, 4, 50]</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16_transformer_encoder.layers.2.Dropout_dropout1</th>\n",
       "      <td>-</td>\n",
       "      <td>[20, 4, 50]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17_transformer_encoder.layers.2.LayerNorm_norm1</th>\n",
       "      <td>[50]</td>\n",
       "      <td>[20, 4, 50]</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18_transformer_encoder.layers.2.Linear_linear1</th>\n",
       "      <td>[50, 2048]</td>\n",
       "      <td>[20, 4, 2048]</td>\n",
       "      <td>104448.0</td>\n",
       "      <td>102400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19_transformer_encoder.layers.2.Dropout_dropout</th>\n",
       "      <td>-</td>\n",
       "      <td>[20, 4, 2048]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20_transformer_encoder.layers.2.Linear_linear2</th>\n",
       "      <td>[2048, 50]</td>\n",
       "      <td>[20, 4, 50]</td>\n",
       "      <td>102450.0</td>\n",
       "      <td>102400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21_transformer_encoder.layers.2.Dropout_dropout2</th>\n",
       "      <td>-</td>\n",
       "      <td>[20, 4, 50]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22_transformer_encoder.layers.2.LayerNorm_norm2</th>\n",
       "      <td>[50]</td>\n",
       "      <td>[20, 4, 50]</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23_transformer_encoder.layers.3.Dropout_dropout1</th>\n",
       "      <td>-</td>\n",
       "      <td>[20, 4, 50]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24_transformer_encoder.layers.3.LayerNorm_norm1</th>\n",
       "      <td>[50]</td>\n",
       "      <td>[20, 4, 50]</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25_transformer_encoder.layers.3.Linear_linear1</th>\n",
       "      <td>[50, 2048]</td>\n",
       "      <td>[20, 4, 2048]</td>\n",
       "      <td>104448.0</td>\n",
       "      <td>102400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26_transformer_encoder.layers.3.Dropout_dropout</th>\n",
       "      <td>-</td>\n",
       "      <td>[20, 4, 2048]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27_transformer_encoder.layers.3.Linear_linear2</th>\n",
       "      <td>[2048, 50]</td>\n",
       "      <td>[20, 4, 50]</td>\n",
       "      <td>102450.0</td>\n",
       "      <td>102400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28_transformer_encoder.layers.3.Dropout_dropout2</th>\n",
       "      <td>-</td>\n",
       "      <td>[20, 4, 50]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29_transformer_encoder.layers.3.LayerNorm_norm2</th>\n",
       "      <td>[50]</td>\n",
       "      <td>[20, 4, 50]</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30_transformer_encoder.layers.4.Dropout_dropout1</th>\n",
       "      <td>-</td>\n",
       "      <td>[20, 4, 50]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31_transformer_encoder.layers.4.LayerNorm_norm1</th>\n",
       "      <td>[50]</td>\n",
       "      <td>[20, 4, 50]</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32_transformer_encoder.layers.4.Linear_linear1</th>\n",
       "      <td>[50, 2048]</td>\n",
       "      <td>[20, 4, 2048]</td>\n",
       "      <td>104448.0</td>\n",
       "      <td>102400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33_transformer_encoder.layers.4.Dropout_dropout</th>\n",
       "      <td>-</td>\n",
       "      <td>[20, 4, 2048]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34_transformer_encoder.layers.4.Linear_linear2</th>\n",
       "      <td>[2048, 50]</td>\n",
       "      <td>[20, 4, 50]</td>\n",
       "      <td>102450.0</td>\n",
       "      <td>102400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35_transformer_encoder.layers.4.Dropout_dropout2</th>\n",
       "      <td>-</td>\n",
       "      <td>[20, 4, 50]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36_transformer_encoder.layers.4.LayerNorm_norm2</th>\n",
       "      <td>[50]</td>\n",
       "      <td>[20, 4, 50]</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37_transformer_encoder.layers.5.Dropout_dropout1</th>\n",
       "      <td>-</td>\n",
       "      <td>[20, 4, 50]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38_transformer_encoder.layers.5.LayerNorm_norm1</th>\n",
       "      <td>[50]</td>\n",
       "      <td>[20, 4, 50]</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39_transformer_encoder.layers.5.Linear_linear1</th>\n",
       "      <td>[50, 2048]</td>\n",
       "      <td>[20, 4, 2048]</td>\n",
       "      <td>104448.0</td>\n",
       "      <td>102400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40_transformer_encoder.layers.5.Dropout_dropout</th>\n",
       "      <td>-</td>\n",
       "      <td>[20, 4, 2048]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41_transformer_encoder.layers.5.Linear_linear2</th>\n",
       "      <td>[2048, 50]</td>\n",
       "      <td>[20, 4, 50]</td>\n",
       "      <td>102450.0</td>\n",
       "      <td>102400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42_transformer_encoder.layers.5.Dropout_dropout2</th>\n",
       "      <td>-</td>\n",
       "      <td>[20, 4, 50]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43_transformer_encoder.layers.5.LayerNorm_norm2</th>\n",
       "      <td>[50]</td>\n",
       "      <td>[20, 4, 50]</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44_fc_out</th>\n",
       "      <td>[50, 17]</td>\n",
       "      <td>[20, 4, 17]</td>\n",
       "      <td>867.0</td>\n",
       "      <td>850.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Kernel Shape  ... Mult-Adds\n",
       "Layer                                                           ...          \n",
       "0_encoder                                         [50, 400002]  ...       NaN\n",
       "1_pos_encoder.Dropout_dropout                                -  ...       NaN\n",
       "2_transformer_encoder.layers.0.Dropout_dropout1              -  ...       NaN\n",
       "3_transformer_encoder.layers.0.LayerNorm_norm1            [50]  ...      50.0\n",
       "4_transformer_encoder.layers.0.Linear_linear1       [50, 2048]  ...  102400.0\n",
       "5_transformer_encoder.layers.0.Dropout_dropout               -  ...       NaN\n",
       "6_transformer_encoder.layers.0.Linear_linear2       [2048, 50]  ...  102400.0\n",
       "7_transformer_encoder.layers.0.Dropout_dropout2              -  ...       NaN\n",
       "8_transformer_encoder.layers.0.LayerNorm_norm2            [50]  ...      50.0\n",
       "9_transformer_encoder.layers.1.Dropout_dropout1              -  ...       NaN\n",
       "10_transformer_encoder.layers.1.LayerNorm_norm1           [50]  ...      50.0\n",
       "11_transformer_encoder.layers.1.Linear_linear1      [50, 2048]  ...  102400.0\n",
       "12_transformer_encoder.layers.1.Dropout_dropout              -  ...       NaN\n",
       "13_transformer_encoder.layers.1.Linear_linear2      [2048, 50]  ...  102400.0\n",
       "14_transformer_encoder.layers.1.Dropout_dropout2             -  ...       NaN\n",
       "15_transformer_encoder.layers.1.LayerNorm_norm2           [50]  ...      50.0\n",
       "16_transformer_encoder.layers.2.Dropout_dropout1             -  ...       NaN\n",
       "17_transformer_encoder.layers.2.LayerNorm_norm1           [50]  ...      50.0\n",
       "18_transformer_encoder.layers.2.Linear_linear1      [50, 2048]  ...  102400.0\n",
       "19_transformer_encoder.layers.2.Dropout_dropout              -  ...       NaN\n",
       "20_transformer_encoder.layers.2.Linear_linear2      [2048, 50]  ...  102400.0\n",
       "21_transformer_encoder.layers.2.Dropout_dropout2             -  ...       NaN\n",
       "22_transformer_encoder.layers.2.LayerNorm_norm2           [50]  ...      50.0\n",
       "23_transformer_encoder.layers.3.Dropout_dropout1             -  ...       NaN\n",
       "24_transformer_encoder.layers.3.LayerNorm_norm1           [50]  ...      50.0\n",
       "25_transformer_encoder.layers.3.Linear_linear1      [50, 2048]  ...  102400.0\n",
       "26_transformer_encoder.layers.3.Dropout_dropout              -  ...       NaN\n",
       "27_transformer_encoder.layers.3.Linear_linear2      [2048, 50]  ...  102400.0\n",
       "28_transformer_encoder.layers.3.Dropout_dropout2             -  ...       NaN\n",
       "29_transformer_encoder.layers.3.LayerNorm_norm2           [50]  ...      50.0\n",
       "30_transformer_encoder.layers.4.Dropout_dropout1             -  ...       NaN\n",
       "31_transformer_encoder.layers.4.LayerNorm_norm1           [50]  ...      50.0\n",
       "32_transformer_encoder.layers.4.Linear_linear1      [50, 2048]  ...  102400.0\n",
       "33_transformer_encoder.layers.4.Dropout_dropout              -  ...       NaN\n",
       "34_transformer_encoder.layers.4.Linear_linear2      [2048, 50]  ...  102400.0\n",
       "35_transformer_encoder.layers.4.Dropout_dropout2             -  ...       NaN\n",
       "36_transformer_encoder.layers.4.LayerNorm_norm2           [50]  ...      50.0\n",
       "37_transformer_encoder.layers.5.Dropout_dropout1             -  ...       NaN\n",
       "38_transformer_encoder.layers.5.LayerNorm_norm1           [50]  ...      50.0\n",
       "39_transformer_encoder.layers.5.Linear_linear1      [50, 2048]  ...  102400.0\n",
       "40_transformer_encoder.layers.5.Dropout_dropout              -  ...       NaN\n",
       "41_transformer_encoder.layers.5.Linear_linear2      [2048, 50]  ...  102400.0\n",
       "42_transformer_encoder.layers.5.Dropout_dropout2             -  ...       NaN\n",
       "43_transformer_encoder.layers.5.LayerNorm_norm2           [50]  ...      50.0\n",
       "44_fc_out                                             [50, 17]  ...     850.0\n",
       "\n",
       "[45 rows x 4 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "summary(model, torch.zeros((20, 4)).long().cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcpirEVGJhff"
   },
   "source": [
    "# Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "YLxGdc8sCd1p"
   },
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    labels = labels.ravel()\n",
    "    mask = (labels >= 0)\n",
    "    outputs = np.argmax(outputs, axis=1)\n",
    "    return np.sum(outputs == labels)/float(np.sum(mask))\n",
    "metrics = {\n",
    "    'accuracy': accuracy,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "S97NLNbNEYxa"
   },
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "def train(model, optimizer, loss_fn, data_iterator, metrics, num_steps):\n",
    "    model.train()\n",
    "    summ = []\n",
    "    t = trange(num_steps)\n",
    "    for i in t:\n",
    "      for train_batch, labels_batch in data_iterator:\n",
    "        output_batch = model(train_batch)\n",
    "        output_batch = output_batch.view(output_batch.shape[0]*output_batch.shape[1], output_batch.shape[2])\n",
    "        labels_batch = labels_batch.view(labels_batch.shape[0]*labels_batch.shape[1],)\n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 40 == 0:\n",
    "            output_batch = output_batch.data.cpu().numpy()\n",
    "            labels_batch = labels_batch.data.cpu().numpy()\n",
    "            summary_batch = {metric: metrics[metric](output_batch, labels_batch)\n",
    "                             for metric in metrics}\n",
    "            summary_batch['loss'] = loss.item()\n",
    "            summ.append(summary_batch)\n",
    "\n",
    "    metrics_mean = {metric: np.mean([x[metric]\n",
    "                                     for x in summ]) for metric in summ[0]}\n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v)\n",
    "                                for k, v in metrics_mean.items())\n",
    "    print(\"- Train metrics: \" + metrics_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "ZA9tkIBcEY0U"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loss_fn, data_iterator, metrics, num_steps):\n",
    "    model.eval()\n",
    "    summ = []\n",
    "    for _ in range(num_steps):\n",
    "      for data_batch, labels_batch  in data_iterator:\n",
    "        output_batch = model(data_batch)\n",
    "        output_batch = output_batch.view(output_batch.shape[0]*output_batch.shape[1], output_batch.shape[2])\n",
    "        labels_batch = labels_batch.view(labels_batch.shape[0]*labels_batch.shape[1],)\n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "        output_batch = output_batch.data.cpu().numpy()\n",
    "        labels_batch = labels_batch.data.cpu().numpy()\n",
    "        summary_batch = {metric: metrics[metric](output_batch, labels_batch)\n",
    "                         for metric in metrics}\n",
    "        summary_batch['loss'] = loss.item()\n",
    "        summ.append(summary_batch)\n",
    "    metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]} \n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items())\n",
    "    print(\"- Eval metrics : \" + metrics_string)\n",
    "    return metrics_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oKblTegTEY3C",
    "outputId": "b6429a7c-5327-414e-c58c-dac05e8235fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.0505e-03, 5.8858e-05, 3.2468e-03, 5.9581e-05, 6.3012e-05, 1.5319e-04,\n",
      "        5.7968e-05, 3.9526e-03, 3.3670e-03, 1.9608e-02, 4.9645e-05, 1.1262e-06,\n",
      "        4.9181e-05, 1.3488e-04, 2.6565e-05, 4.9751e-03, 2.4876e-03],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "total_tags = 0\n",
    "for tag in tag2idx.keys():\n",
    "    total_tags += count_tags[tag]\n",
    "class_weights = [0 for _ in range(unique_tags)]\n",
    "for tag, idx in tag2idx.items():\n",
    "    class_weights[idx] = 1/count_tags[tag]\n",
    "class_weights = torch.FloatTensor(class_weights).cuda()\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "6kK0uAbiEY5E"
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "def train_and_evaluate(model, train_loader, test_loader, optimizer, metrics):\n",
    "    loss_fn = nn.CrossEntropyLoss(class_weights, ignore_index=-1, reduction='mean')\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, num_epochs))\n",
    "        num_steps = (len(train_tokens) + 1) // batch_size\n",
    "        train(model, optimizer, loss_fn, train_loader, metrics, num_steps)\n",
    "        num_steps = (len(test_tokens) + 1) // batch_size\n",
    "        val_metrics = evaluate(model, loss_fn, test_loader, metrics, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "PBL9bzxrdGT-"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bOuzSqSNdGYS",
    "outputId": "9facc663-14a2-461e-95ce-cd2f897324bd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/65 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  2%|▏         | 1/65 [00:16<17:09, 16.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 2/65 [00:31<16:48, 16.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▍         | 3/65 [00:48<16:37, 16.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 4/65 [01:04<16:18, 16.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 5/65 [01:20<15:59, 15.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 6/65 [01:35<15:42, 15.97s/it]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 7/65 [01:51<15:25, 15.95s/it]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 8/65 [02:07<15:08, 15.93s/it]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 9/65 [02:23<14:51, 15.92s/it]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 10/65 [02:39<14:35, 15.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 11/65 [02:55<14:19, 15.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 12/65 [03:11<14:02, 15.90s/it]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 13/65 [03:27<13:46, 15.89s/it]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 14/65 [03:43<13:30, 15.89s/it]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 15/65 [03:58<13:14, 15.89s/it]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▍       | 16/65 [04:14<12:58, 15.89s/it]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 17/65 [04:30<12:43, 15.90s/it]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 18/65 [04:46<12:27, 15.90s/it]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 19/65 [05:02<12:11, 15.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███       | 20/65 [05:18<11:55, 15.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 21/65 [05:34<11:39, 15.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 22/65 [05:50<11:24, 15.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▌      | 23/65 [06:06<11:08, 15.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 24/65 [06:22<10:56, 16.02s/it]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 25/65 [06:38<10:39, 15.99s/it]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 26/65 [06:54<10:22, 15.96s/it]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 27/65 [07:10<10:05, 15.95s/it]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 28/65 [07:26<09:49, 15.93s/it]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▍     | 29/65 [07:42<09:33, 15.93s/it]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 30/65 [07:57<09:17, 15.92s/it]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 31/65 [08:13<09:01, 15.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████▉     | 32/65 [08:29<08:45, 15.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████     | 33/65 [08:45<08:29, 15.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 34/65 [09:01<08:13, 15.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▍    | 35/65 [09:17<07:57, 15.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▌    | 36/65 [09:33<07:41, 15.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 37/65 [09:49<07:25, 15.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 38/65 [10:05<07:09, 15.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 39/65 [10:21<06:53, 15.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 40/65 [10:36<06:37, 15.90s/it]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 41/65 [10:53<06:23, 15.98s/it]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▍   | 42/65 [11:08<06:06, 15.93s/it]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▌   | 43/65 [11:24<05:50, 15.92s/it]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 44/65 [11:41<05:36, 16.03s/it]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▉   | 45/65 [11:57<05:19, 15.99s/it]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 46/65 [12:12<05:03, 15.96s/it]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 47/65 [12:28<04:46, 15.94s/it]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████▍  | 48/65 [12:44<04:30, 15.93s/it]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▌  | 49/65 [13:00<04:14, 15.92s/it]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 50/65 [13:16<03:58, 15.92s/it]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 51/65 [13:32<03:42, 15.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 52/65 [13:48<03:26, 15.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 53/65 [14:04<03:10, 15.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 54/65 [14:20<02:54, 15.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▍ | 55/65 [14:36<02:39, 15.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████▌ | 56/65 [14:51<02:23, 15.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 57/65 [15:07<02:07, 15.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 89%|████████▉ | 58/65 [15:23<01:51, 15.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 91%|█████████ | 59/65 [15:39<01:35, 15.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 60/65 [15:55<01:19, 15.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 61/65 [16:11<01:03, 15.90s/it]\u001b[A\u001b[A\n",
      "\n",
      " 95%|█████████▌| 62/65 [16:27<00:47, 15.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 63/65 [16:43<00:31, 15.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████▊| 64/65 [16:59<00:15, 15.91s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 65/65 [17:15<00:00, 15.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train metrics: accuracy: 0.002 ; loss: 00nan\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-eca9122365eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-118-301e53b1662e>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, train_loader, test_loader, optimizer, metrics)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mval_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-116-5dd0a2c28956>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, loss_fn, data_iterator, metrics, num_steps)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_batch\u001b[0m  \u001b[0;32min\u001b[0m \u001b[0mdata_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0moutput_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0moutput_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moutput_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mlabels_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlabels_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-108-ac02739423a4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    292\u001b[0m         \"\"\"\n\u001b[1;32m    293\u001b[0m         src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n\u001b[0;32m--> 294\u001b[0;31m                               key_padding_mask=src_key_padding_mask)[0]\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[1;32m    985\u001b[0m                 \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m                 \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneed_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 987\u001b[0;31m                 attn_mask=attn_mask)\n\u001b[0m\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[1;32m   4801\u001b[0m         attn_output_weights = attn_output_weights.masked_fill(\n\u001b[1;32m   4802\u001b[0m             \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4803\u001b[0;31m             \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4804\u001b[0m         )\n\u001b[1;32m   4805\u001b[0m         \u001b[0mattn_output_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 520.00 MiB (GPU 0; 15.78 GiB total capacity; 12.23 GiB already allocated; 428.75 MiB free; 14.07 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(model, train_loader, test_loader, optimizer, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ASmKdSPmCM_"
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "def create_download_link(title = \"Download model file\", filename = \"./images.zip\"):  \n",
    "    html = '<a href={filename}>{title}</a>'\n",
    "    html = html.format(title=title,filename=filename)\n",
    "    return HTML(html)\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), \"model_100.pth\")\n",
    "create_download_link(filename='./model_100.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7JRw7L2mCQ0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Named_Entity_Recognition_PyTorch_Transformers.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
