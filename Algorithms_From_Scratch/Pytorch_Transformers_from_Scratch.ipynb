{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pytorch_Transformers_from_Scratch.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyN+WuYldN8Uob5vyEiWnRd5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"jBP2d6wYZTuW","executionInfo":{"status":"ok","timestamp":1624440917997,"user_tz":-330,"elapsed":2917,"user":{"displayName":"hemant sharma","photoUrl":"","userId":"09562798786707226656"}}},"source":["import torch\n","import torch.nn as nn\n","import numpy as np"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"8pwqma-xdTyE"},"source":["class SelfAttention(nn.Module):\n","    def __init__(self, embed_size, heads):\n","        super(SelfAttention, self).__init__()\n","        self.embed_size = embed_size\n","        self.heads = heads\n","        self.head_dim = embed_size // heads\n","\n","        assert (\n","            self.head_dim * heads == embed_size\n","        ), \"Embedding size needs to be divisible by heads\"\n","\n","        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n","\n","    def forward(self, values, keys, query, mask):\n","        # Get number of training examples\n","        N = query.shape[0]\n","\n","        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n","\n","        # Split the embedding into self.heads different pieces\n","        values = values.reshape(N, value_len, self.heads, self.head_dim)\n","        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n","        query = query.reshape(N, query_len, self.heads, self.head_dim)\n","\n","        values = self.values(values)  # (N, value_len, heads, head_dim)\n","        keys = self.keys(keys)  # (N, key_len, heads, head_dim)\n","        queries = self.queries(query)  # (N, query_len, heads, heads_dim)\n","\n","        # Einsum does matrix mult. for query*keys for each training example\n","        # with every other training example, don't be confused by einsum\n","        # it's just how I like doing matrix multiplication & bmm\n","\n","        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n","        # queries shape: (N, query_len, heads, heads_dim),\n","        # keys shape: (N, key_len, heads, heads_dim)\n","        # energy: (N, heads, query_len, key_len)\n","\n","        # Mask padded indices so their weights become 0\n","        if mask is not None:\n","            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n","\n","        # Normalize energy values similarly to seq2seq + attention\n","        # so that they sum to 1. Also divide by scaling factor for\n","        # better stability\n","        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n","        # attention shape: (N, heads, query_len, key_len)\n","\n","        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n","            N, query_len, self.heads * self.head_dim\n","        )\n","        # attention shape: (N, heads, query_len, key_len)\n","        # values shape: (N, value_len, heads, heads_dim)\n","        # out after matrix multiply: (N, query_len, heads, head_dim), then\n","        # we reshape and flatten the last two dimensions.\n","\n","        out = self.fc_out(out)\n","        # Linear layer doesn't modify the shape, final shape will be\n","        # (N, query_len, embed_size)\n","\n","        return out\n","\n","\n","class TransformerBlock(nn.Module):\n","    def __init__(self, embed_size, heads, dropout, forward_expansion):\n","        super(TransformerBlock, self).__init__()\n","        self.attention = SelfAttention(embed_size, heads)\n","        self.norm1 = nn.LayerNorm(embed_size)\n","        self.norm2 = nn.LayerNorm(embed_size)\n","\n","        self.feed_forward = nn.Sequential(\n","            nn.Linear(embed_size, forward_expansion * embed_size),\n","            nn.ReLU(),\n","            nn.Linear(forward_expansion * embed_size, embed_size),\n","        )\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, value, key, query, mask):\n","        attention = self.attention(value, key, query, mask)\n","\n","        # Add skip connection, run through normalization and finally dropout\n","        x = self.dropout(self.norm1(attention + query))\n","        forward = self.feed_forward(x)\n","        out = self.dropout(self.norm2(forward + x))\n","        return out\n","\n","class PositionalEncoder(nn.Module):\n","    def __init__(self, d_model, max_seq_len = 80):\n","        super().__init__()\n","        self.d_model = d_model\n","        \n","        # create constant 'pe' matrix with values dependant on \n","        # pos and i\n","        pe = torch.zeros(max_seq_len, d_model)\n","        for pos in range(max_seq_len):\n","            for i in range(0, d_model, 2):\n","                pe[pos, i] = \\\n","                math.sin(pos / (10000 ** ((2 * i)/d_model)))\n","                pe[pos, i + 1] = \\\n","                math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n","                \n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n"," \n","    \n","    def forward(self, x):\n","        # make embeddings relatively larger\n","        x = x * math.sqrt(self.d_model)\n","        #add constant to embedding\n","        seq_len = x.size(1)\n","        x = x + Variable(self.pe[:,:seq_len], \\\n","        requires_grad=False).cuda()\n","        return x\n","\n","class Encoder(nn.Module):\n","    def __init__(\n","        self,\n","        src_vocab_size,\n","        embed_size,\n","        num_layers,\n","        heads,\n","        device,\n","        forward_expansion,\n","        dropout,\n","        max_length,\n","    ):\n","\n","        super(Encoder, self).__init__()\n","        self.embed_size = embed_size\n","        self.device = device\n","        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n","        self.position_embedding = PositionalEncoder( embed_size, max_seq_len = max_length)\n","\n","        self.layers = nn.ModuleList(\n","            [\n","                TransformerBlock(\n","                    embed_size,\n","                    heads,\n","                    dropout=dropout,\n","                    forward_expansion=forward_expansion,\n","                )\n","                for _ in range(num_layers)\n","            ]\n","        )\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, mask):\n","        N, seq_length = x.shape\n","        #positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n","        x = self.word_embedding(x)\n","        out = self.position_embedding(x)\n","        \n","        # In the Encoder the query, key, value are all the same, it's in the\n","        # decoder this will change. This might look a bit odd in this case.\n","        for layer in self.layers:\n","            out = layer(out, out, out, mask)\n","\n","        return out\n","\n","\n","class DecoderBlock(nn.Module):\n","    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n","        super(DecoderBlock, self).__init__()\n","        self.norm = nn.LayerNorm(embed_size)\n","        self.attention = SelfAttention(embed_size, heads=heads)\n","        self.transformer_block = TransformerBlock(\n","            embed_size, heads, dropout, forward_expansion\n","        )\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, value, key, src_mask, trg_mask):\n","        attention = self.attention(x, x, x, trg_mask)\n","        query = self.dropout(self.norm(attention + x))\n","        out = self.transformer_block(value, key, query, src_mask)\n","        return out\n","\n","\n","class Decoder(nn.Module):\n","    def __init__(\n","        self,\n","        trg_vocab_size,\n","        embed_size,\n","        num_layers,\n","        heads,\n","        forward_expansion,\n","        dropout,\n","        device,\n","        max_length,\n","    ):\n","        super(Decoder, self).__init__()\n","        self.device = device\n","        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n","        self.position_embedding = PositionalEncoder( embed_size, max_seq_len = max_length)\n","        self.layers = nn.ModuleList(\n","            [\n","                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n","                for _ in range(num_layers)\n","            ]\n","        )\n","        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, enc_out, src_mask, trg_mask):\n","        N, seq_length = x.shape\n","        #positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n","        x = self.word_embedding(x)\n","        x = self.position_embedding(x)\n","        for layer in self.layers:\n","            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n","\n","        out = self.fc_out(x)\n","\n","        return out\n","\n","\n","class Transformer(nn.Module):\n","    def __init__(\n","        self,\n","        src_vocab_size,\n","        trg_vocab_size,\n","        src_pad_idx,\n","        trg_pad_idx,\n","        embed_size=512,\n","        num_layers=6,\n","        forward_expansion=4,\n","        heads=8,\n","        dropout=0,\n","        device=\"cpu\",\n","        max_length=100,\n","    ):\n","\n","        super(Transformer, self).__init__()\n","\n","        self.encoder = Encoder(\n","            src_vocab_size,\n","            embed_size,\n","            num_layers,\n","            heads,\n","            device,\n","            forward_expansion,\n","            dropout,\n","            max_length,\n","        )\n","\n","        self.decoder = Decoder(\n","            trg_vocab_size,\n","            embed_size,\n","            num_layers,\n","            heads,\n","            forward_expansion,\n","            dropout,\n","            device,\n","            max_length,\n","        )\n","\n","        self.src_pad_idx = src_pad_idx\n","        self.trg_pad_idx = trg_pad_idx\n","        self.device = device\n","\n","    def make_src_mask(self, src):\n","        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n","        # (N, 1, 1, src_len)\n","        return src_mask.to(self.device)\n","    \n","    def make_trg_mask(self, trg):\n","\n","        N, trg_len = trg.shape\n","        \n","        target_msk = (target_seq != self.trg_pad_idx).unsqueeze(1)\n","\n","        nopeak_mask = np.triu(np.ones(1, trg_len, trg_len),k=1).astype('uint8')\n","        nopeak_mask = Variable(torch.from_numpy(nopeak_mask) == 0)\n","        trg_msk = target_msk & nopeak_mask\n","\n","        return trg_mask.to(self.device)\n","\n","\n","    def make_trg_mask(self, trg):\n","        N, trg_len = trg.shape\n","        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n","            N, 1, trg_len, trg_len\n","        )\n","\n","        return trg_mask.to(self.device)\n","\n","    def forward(self, src, trg):\n","        src_mask = self.make_src_mask(src)\n","        trg_mask = self.make_trg_mask(trg)\n","        enc_src = self.encoder(src, src_mask)\n","        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n","        return out\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hn9WrNqha1rK","executionInfo":{"status":"ok","timestamp":1624440933576,"user_tz":-330,"elapsed":365,"user":{"displayName":"hemant sharma","photoUrl":"","userId":"09562798786707226656"}},"outputId":"3955f97a-5371-4b92-8cf5-4c2eb841cced"},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","   "],"execution_count":2,"outputs":[{"output_type":"stream","text":["cpu\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ae5SsDj9a5aX","executionInfo":{"status":"ok","timestamp":1624440933883,"user_tz":-330,"elapsed":3,"user":{"displayName":"hemant sharma","photoUrl":"","userId":"09562798786707226656"}}},"source":["x = torch.tensor([[1, 5, 6, 4, 3, 9, 2, 2, 2], [1, 8, 7, 3, 4, 5, 6, 2, 2]]).to(device)\n","trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XtebTZo2a-Ao","executionInfo":{"status":"ok","timestamp":1624440936439,"user_tz":-330,"elapsed":411,"user":{"displayName":"hemant sharma","photoUrl":"","userId":"09562798786707226656"}},"outputId":"a3e0b7a9-5fec-4ee3-8291-473a4d1872d5"},"source":["print(x)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["tensor([[1, 5, 6, 4, 3, 9, 2, 2, 2],\n","        [1, 8, 7, 3, 4, 5, 6, 2, 2]])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d0RWONbKbIqy","executionInfo":{"status":"ok","timestamp":1624440937412,"user_tz":-330,"elapsed":548,"user":{"displayName":"hemant sharma","photoUrl":"","userId":"09562798786707226656"}}},"source":["input_pad = 2"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"5nUpbFECbIzS","executionInfo":{"status":"ok","timestamp":1624440937724,"user_tz":-330,"elapsed":2,"user":{"displayName":"hemant sharma","photoUrl":"","userId":"09562798786707226656"}}},"source":["input_msk = (x != input_pad).unsqueeze(1).unsqueeze(2)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w3nyXkWbbZc_","executionInfo":{"status":"ok","timestamp":1624440938577,"user_tz":-330,"elapsed":3,"user":{"displayName":"hemant sharma","photoUrl":"","userId":"09562798786707226656"}},"outputId":"b7c60202-0f56-4759-ff6a-d116d6173b88"},"source":["print(input_msk)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["tensor([[[[ True,  True,  True,  True,  True,  True, False, False, False]]],\n","\n","\n","        [[[ True,  True,  True,  True,  True,  True,  True, False, False]]]])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UZmHgQiicgi_","executionInfo":{"status":"ok","timestamp":1624440940343,"user_tz":-330,"elapsed":2,"user":{"displayName":"hemant sharma","photoUrl":"","userId":"09562798786707226656"}}},"source":["N, trg_len = trg.shape\n","trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(N, 1, trg_len, trg_len)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G08xui4ScmyT","executionInfo":{"status":"ok","timestamp":1624440941269,"user_tz":-330,"elapsed":310,"user":{"displayName":"hemant sharma","photoUrl":"","userId":"09562798786707226656"}},"outputId":"22597b06-5cbf-45c6-9bc3-62b9dbf3a4f8"},"source":["print(trg_mask)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["tensor([[[[1., 0., 0., 0., 0., 0., 0., 0.],\n","          [1., 1., 0., 0., 0., 0., 0., 0.],\n","          [1., 1., 1., 0., 0., 0., 0., 0.],\n","          [1., 1., 1., 1., 0., 0., 0., 0.],\n","          [1., 1., 1., 1., 1., 0., 0., 0.],\n","          [1., 1., 1., 1., 1., 1., 0., 0.],\n","          [1., 1., 1., 1., 1., 1., 1., 0.],\n","          [1., 1., 1., 1., 1., 1., 1., 1.]]],\n","\n","\n","        [[[1., 0., 0., 0., 0., 0., 0., 0.],\n","          [1., 1., 0., 0., 0., 0., 0., 0.],\n","          [1., 1., 1., 0., 0., 0., 0., 0.],\n","          [1., 1., 1., 1., 0., 0., 0., 0.],\n","          [1., 1., 1., 1., 1., 0., 0., 0.],\n","          [1., 1., 1., 1., 1., 1., 0., 0.],\n","          [1., 1., 1., 1., 1., 1., 1., 0.],\n","          [1., 1., 1., 1., 1., 1., 1., 1.]]]])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oK4ydMO7sjeE","executionInfo":{"status":"ok","timestamp":1624441083758,"user_tz":-330,"elapsed":316,"user":{"displayName":"hemant sharma","photoUrl":"","userId":"09562798786707226656"}}},"source":["def make_trg_mask(trg):\n","\n","        N, trg_len = trg.shape\n","        \n","        target_msk = (target_seq != trg_pad_idx).unsqueeze(1)\n","\n","        nopeak_mask = np.triu(np.ones(1, trg_len, trg_len),k=1).astype('uint8')\n","        nopeak_mask = Variable(torch.from_numpy(nopeak_mask) == 0)\n","        trg_msk = target_msk & nopeak_mask\n","\n","        return trg_mask.to(device)\n","      "],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"FU1x1VyrsjjQ","colab":{"base_uri":"https://localhost:8080/","height":283},"executionInfo":{"status":"error","timestamp":1624441085765,"user_tz":-330,"elapsed":390,"user":{"displayName":"hemant sharma","photoUrl":"","userId":"09562798786707226656"}},"outputId":"54f9ff8a-ad3b-4539-806a-e67c379cf5b9"},"source":["z = make_trg_mask(trg)"],"execution_count":14,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-125555a5624a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_trg_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-13-c0edef77afee>\u001b[0m in \u001b[0;36mmake_trg_mask\u001b[0;34m(trg)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtarget_msk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget_seq\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtrg_pad_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mnopeak_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'target_seq' is not defined"]}]},{"cell_type":"code","metadata":{"id":"DFhfmXtEsjl7"},"source":[""],"execution_count":null,"outputs":[]}]}