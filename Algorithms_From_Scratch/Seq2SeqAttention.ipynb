{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Seq2SeqAttention.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNFs2St4extQyYsmv1R7M9G"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"fQ5pC5upK5Gw"},"source":["class Encoder(nn.Module):\n","    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n","        super(Encoder, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","\n","        self.embedding = nn.Embedding(input_size, embedding_size)\n","        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, bidirectional=True)\n","\n","        self.fc_hidden = nn.Linear(hidden_size * 2, hidden_size)\n","        self.fc_cell = nn.Linear(hidden_size * 2, hidden_size)\n","        self.dropout = nn.Dropout(p)\n","\n","    def forward(self, x):\n","        # x: (seq_length, N) where N is batch size\n","\n","        embedding = self.dropout(self.embedding(x))\n","        # embedding shape: (seq_length, N, embedding_size)\n","\n","        encoder_states, (hidden, cell) = self.rnn(embedding)\n","        # outputs shape: (seq_length, N, hidden_size)\n","\n","        # Use forward, backward cells and hidden through a linear layer\n","        # so that it can be input to the decoder which is not bidirectional\n","        # Also using index slicing ([idx:idx+1]) to keep the dimension\n","        hidden = self.fc_hidden(torch.cat((hidden[0:1], hidden[1:2]), dim=2))\n","        cell = self.fc_cell(torch.cat((cell[0:1], cell[1:2]), dim=2))\n","\n","        return encoder_states, hidden, cell\n","\n","\n","class Decoder(nn.Module):\n","    def __init__(\n","        self, input_size, embedding_size, hidden_size, output_size, num_layers, p\n","    ):\n","        super(Decoder, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","\n","        self.embedding = nn.Embedding(input_size, embedding_size)\n","        self.rnn = nn.LSTM(hidden_size * 2 + embedding_size, hidden_size, num_layers)\n","\n","        self.energy = nn.Linear(hidden_size * 3, 1)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","        self.dropout = nn.Dropout(p)\n","        self.softmax = nn.Softmax(dim=0)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x, encoder_states, hidden, cell):\n","        x = x.unsqueeze(0)\n","        # x: (1, N) where N is the batch size\n","\n","        embedding = self.dropout(self.embedding(x))\n","        # embedding shape: (1, N, embedding_size)\n","\n","        sequence_length = encoder_states.shape[0]\n","        h_reshaped = hidden.repeat(sequence_length, 1, 1)\n","        # h_reshaped: (seq_length, N, hidden_size*2)\n","\n","        energy = self.relu(self.energy(torch.cat((h_reshaped, encoder_states), dim=2)))\n","        # energy: (seq_length, N, 1)\n","\n","        attention = self.softmax(energy)\n","        # attention: (seq_length, N, 1)\n","\n","        # attention: (seq_length, N, 1), snk\n","        # encoder_states: (seq_length, N, hidden_size*2), snl\n","        # we want context_vector: (1, N, hidden_size*2), i.e knl\n","        context_vector = torch.einsum(\"snk,snl->knl\", attention, encoder_states)\n","\n","        rnn_input = torch.cat((context_vector, embedding), dim=2)\n","        # rnn_input: (1, N, hidden_size*2 + embedding_size)\n","\n","        outputs, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n","        # outputs shape: (1, N, hidden_size)\n","\n","        predictions = self.fc(outputs).squeeze(0)\n","        # predictions: (N, hidden_size)\n","\n","        return predictions, hidden, cell\n","\n","\n","class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super(Seq2Seq, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, source, target, teacher_force_ratio=0.5):\n","        batch_size = source.shape[1]\n","        target_len = target.shape[0]\n","        target_vocab_size = len(french.vocab)\n","\n","        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n","        encoder_states, hidden, cell = self.encoder(source)\n","\n","        # First input will be <SOS> token\n","        x = target[0]\n","\n","        for t in range(1, target_len):\n","            # At every time step use encoder_states and update hidden, cell\n","            output, hidden, cell = self.decoder(x, encoder_states, hidden, cell)\n","\n","            # Store prediction for current time step\n","            outputs[t] = output\n","\n","            # Get the best word the Decoder predicted (index in the vocabulary)\n","            best_guess = output.argmax(1)\n","\n","            # With probability of teacher_force_ratio we take the actual next word\n","            # otherwise we take the word that the Decoder predicted it to be.\n","            # Teacher Forcing is used so that the model gets used to seeing\n","            # similar inputs at training and testing time, if teacher forcing is 1\n","            # then inputs at test time might be completely different than what the\n","            # network is used to. This was a long comment.\n","            x = target[t] if random.random() < teacher_force_ratio else best_guess\n","\n","        return outputs"],"execution_count":null,"outputs":[]}]}