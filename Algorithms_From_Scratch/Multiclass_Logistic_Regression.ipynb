{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Multiclass_Logistic_Regression.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMRKtTQoGrPY1MVzzLyEoNA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"U9gfGMFDLi0g"},"source":["from scipy.special import softmax\n","\n","def convert_to_one_hot(Y, C):\n","    Y = np.eye(C)[Y.reshape(-1)]\n","    return Y\n","\n","def multi_logistic_model(X, Y, X_val, Y_val, learning_rate = 0.01, num_iterations = 400 ):\n","    \n","    np.random.seed(1)\n","    lamda = 0.1\n","    m = Y.shape[0]                          # number of training examples\n","    n_y = 6                                 # number of classes  \n","    n_h = 50                                # dimensions of the GloVe vectors \n","    \n","    # Initialize parameters using Xavier initialization\n","    W = np.random.randn(n_h, n_y) / np.sqrt(n_h)\n","    b = np.zeros((n_y,))\n","    theta = np.vstack((W,b))\n","  \n","    X = np.hstack((X, np.ones((X.shape[0], 1), dtype=X.dtype)))\n","    X_val = np.hstack((X_val, np.ones((X_val.shape[0], 1), dtype=X_val.dtype)))\n","    Y_oh = convert_to_one_hot(Y, C = n_y)\n","\n","    step_list = [] \n","    loss_list = []\n","    train_accuracy_list = []\n","    val_accuracy_list = []\n","     \n","\n","\n","\n","    #def getLoss(w,x,y,lam):\n","    #m = x.shape[0] #First we get the number of training examples\n","    #y_mat = oneHotIt(y) #Next we convert the integer class coding into a one-hot representation\n","    #scores = np.dot(x,w) #Then we compute raw class scores given our input and current weights\n","    #prob = softmax(scores) #Next we perform a softmax on these scores to get their probabilities\n","    #loss = (-1 / m) * np.sum(y_mat * np.log(prob)) + (lam/2)*np.sum(w*w) #We then find the loss of the probabilities\n","    #grad = (-1 / m) * np.dot(x.T,(y_mat - prob)) + lam*w #And compute the gradient for that loss\n","    #return loss,grad\n","\n","\n","\n","    for t in range(num_iterations+1):\n","      z = np.dot(X,theta)\n","      a = softmax(z)\n","\n","      cost = (-1/m)*(np.sum(Y_oh*np.log(a))) #+ (lamda/2)*np.sum(theta*theta)\n","           \n","      d_theta = (-1/m)*(np.dot(X.T, (Y_oh - a))) #+ lamda*theta\n","\n","      theta = theta - learning_rate * d_theta\n","      \n","      step_list.append(t) \n","      loss_list.append(cost)\n","          \n","      train_accuracy_list.append(accuracy(X, Y, theta))\n","      val_accuracy_list.append(accuracy(X_val, Y_val, theta))\n","\n","      if t % 100 == 0:\n","        #learning_rate = learning_rate/(1 + 0.001*t)\n","        #print('learning rate is ' + str(learning_rate))\n","        print(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n","        print(\"train_accuracy:\" + str(accuracy(X, Y, theta)))\n","        print(\"val_accuracy:\" + str(accuracy(X_val, Y_val, theta)))\n","      \n","\n","      df = pd.DataFrame({\n","        'step': step_list, \n","        'loss': loss_list,\n","        'train_accuracy' : train_accuracy_list,\n","        'val_accuracy' : val_accuracy_list\n","        \n","\n","    })\n","    return df, theta\n","         \n","\n","def predict(X, theta):\n","  m = X.shape[0]\n","  pred = np.zeros((m, 1))\n","  for j in range(m):                       \n","        Z = np.dot(X[j],theta)\n","        A = softmax(Z)\n","        pred[j] = np.argmax(A)\n","  return pred\n","\n","\n","def accuracy(X, Y, theta):\n","    m = X.shape[0]\n","    pred = predict(X, theta)\n","    Accuracy = np.mean((pred[:] == Y.reshape(Y.shape[0],1)[:]))\n","    return Accuracy\n","    \n","\n","def loss_plot(df):\n","    return df.plot(\n","            x='step', \n","            y='loss',\n","            xlabel='step',\n","            ylabel='loss'\n","        )  \n","    \n","def accuracy_plot(df):\n","    return df.plot(\n","            x='step', \n","            y=['train_accuracy', 'val_accuracy'],\n","            xlabel='step',\n","            ylabel='Accuracy'\n","        )  "],"execution_count":null,"outputs":[]}]}